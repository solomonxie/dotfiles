#######################################################################
#                               COMMON                                #
#######################################################################
snippet ifmain
def main():
	pass


if __name__ == '__main__':
	main()
endsnippet

snippet jsonload
with open('${1:${VISUAL:path}}', 'r') as f:
	${2:${VISUAL:data}} = json.loads(f.read())
endsnippet

snippet jsondump
with open('${1:${VISUAL:path}}', 'w') as f:
	f.write(json.dumps(${2:${VISUAL:data}}))
endsnippet

snippet parse-url
import urllib
urlobj = urllib.parse.urlparse(url)
endsnippet

snippet parse-url-params
import urllib
urlobj = urllib.parse.urlparse(url)
params = urllib.parse.parse_qs(urlobj.query)
endsnippet

snippet	shuffle_list
shuffled_list = sorted(${1:${VISUAL:target_list}}, key=lambda _: random.random())
endsnippet

snippet retry "from retry import retry"
@retry((KeyError, RuntimeError), tries=5, delay=2, jitter=5)
endsnippet

snippet deepcopy
from copy import deepcopy
endsnippet

snippet product
from itertools import product
endsnippet

#######################################################################
#                              DEBUGGING                              #
#######################################################################
snippet pdb
__import__('pdb').set_trace()
endsnippet

snippet ipdb
__import__('ipdb').set_trace()
endsnippet

snippet pp
__import__('pprint').pprint(${1})
endsnippet

snippet pprint
__import__('pprint').pprint(${1})
endsnippet

snippet sleep
__import__('time').sleep(${1})
endsnippet

snippet snoop
with __import__('pysnooper').snoop():
	${0:put_code_under_here_for_snooping}
endsnippet


snippet logger
from logging import getLogger
logger = getLogger(__name__)
endsnippet


#######################################################################
#                             DATE / TIME                             #
#######################################################################
snippet now
from datetime import datetime, timedelta
datetime.now().strftime('%Y%m%d%H%M%S')
endsnippet

snippet date-range
from datetime import datetime, timedelta
def get_dates_from_range(granularity, start_date, end_date):
	"""
		print(get_dates_from_range('monthly', '2019-01-01', '2019-10-20'), '\n\n')
		print(get_dates_from_range('weekly', '2019-01-01', '2019-10-20'), '\n\n')
		print(get_dates_from_range('daily', '2019-01-01', '2019-01-20'), '\n\n')
		[('2018-12-30', 'weekly'), ('2019-01-06', 'weekly'), ('2019-01-01', 'daily')....]
	"""
	start, end = datetime.strptime(start_date, '%Y-%m-%d'), datetime.strptime(end_date, '%Y-%m-%d')
	daily_dates = [start + timedelta(days=i) for i in range((end - start).days + 1)]

	target_dates = []
	if granularity == 'daily':
		target_dates = daily_dates
	elif granularity == 'weekly':
		# Start from Monday: weeday_index = day.weekday()
		# Start from Sunday: weeday_index = (day.weekday() + 1) % 7
		target_dates = sorted(set([day - timedelta(days=(day.weekday() + 1) % 7) for day in daily_dates]))  # Sundays
	elif granularity == 'monthly':
		target_dates = sorted(set([day.replace(day=1) for day in daily_dates]))

	combos = [(day.strftime('%Y-%m-%d'), granularity) for day in target_dates]
	return combos
endsnippet

snippet date-to-start-end-date
def get_start_end_date(target_date, granularity):
	target = datetime.strptime(target_date, '%Y-%m-%d') if isinstance(target_date, str) else target_date
	if granularity == 'daily':
		start = end = target
	elif granularity == 'weekly':
		start = target
		end = target + timedelta(days=6)
	elif granularity == 'monthly':
		start = target.replace(day=1)
		any_day_in_next_month = target.replace(day=28) + timedelta(days=4)
		end = any_day_in_next_month - timedelta(days=any_day_in_next_month.day)
	return start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d')
endsnippet


#######################################################################
#                              FILE I/O                               #
#######################################################################
snippet csv_reader
with open('path', 'r') as f:
	reader = __import__('csv').DictWriter(f, fieldnames=cols)
	for row_dict in reader:
		print(row_dict)
endsnippet

snippet csv_writer
cols = ['col1', 'col2']
buff = __import__('io').StringIO()
writer = __import__('csv').DictWriter(buff, fieldnames=cols)
writer.writeheader()
writer.writerow({'name': ['psycho1', 'psycho2'], 'age': [10, 20]})
writer.writerows([{'name': 'jason', 'age': 18}, {'name': 'tom', 'age': 20}])
print(buff.getvalue())
endsnippet

snippet string_io
buff = __import__('io').StringIO()
buff.write('hello')
print(buffer.getvalue())
endsnippet

snippet buffer_io
buff = __import__('io').BytesIO()
buff.write(b'hello')
print(buffer.getvalue())
endsnippet

snippet file_io
endsnippet

snippet gzip_compress
content = __import__('gzip').compress(b'${1:${VISUAL:hello}}', compresslevel=9)
endsnippet


#######################################################################
#                           TEST FRAMEWORK                            #
#######################################################################
snippet mock_constant
patch('${1:module.module.constant_name}', '${2:mocked_value}').start()
endsnippet

snippet mock_method
patch('${1:module.module.method}', MagicMock(return_value=${2:mocked_value})).start()
endsnippet

snippet mock_property
patch(${1:module.module.class_name}, '${2:property_name}', ${3:expected_value}).start()
endsnippet

snippet mock_exception
patch('${1:module.module.method_name}', MagicMock(side_effect=Exception('${2:err_msg}'))).start()
endsnippet


snippet testcase
from unittest import TestCase


class Test${1:${VISUAL:Name}}(TestCase):
	def setUp(self):
		pass

	def tearDown(self):
		pass

	def test_${2:${VISUAL:case_name}}(self):
		self.assertEqual(1, 1)
endsnippet


snippet patch
from unittest.mock import patch, MagicMock
@patch('${1:${VISUAL:module_path}}',
		MagicMock(return_value=${2:${VISUAL:mock_data}}))
endsnippet

#######################################################################
#                     SQLAlchemy - DB OPERATIONS                      #
#######################################################################



#######################################################################
#                         Pandas - DATAFRAME                          #
#######################################################################



#######################################################################
#                              AWS - S3                               #
#######################################################################

snippet s3_client
import boto3
def get_s3_client():
	return boto3.client(
		's3',
		aws_access_key_id='${1:{VISUAL:key_id}}',
		aws_secret_access_key='${2:{VISUAL:secret}}',
		# config=boto3.session.Config(signature_version='s3v4'),
		# endpoint_url='http://your-own-endpoint',
		# region_name='your-region',
	)
endsnippet

snippet s3_resource
import boto3
def get_s3_resource():
	return boto3.resource(
		's3',
		aws_access_key_id='${1:{VISUAL:key_id}}',
		aws_secret_access_key='${2:{VISUAL:secret}}',
		# config=boto3.session.Config(signature_version='s3v4'),
		# endpoint_url='http://your-own-endpoint',
		# region_name='your-region',
	)
endsnippet


snippet s3_create_bucket_if_not_exist
def create_bucket_if_not_exists(bucket):
	s3 = get_s3_resource()
	if not s3.Bucket(bucket).creation_date:
		info = s3.create_bucket(Bucket=bucket)
		print('Created bucket:', info)
	return s3.Bucket(bucket).creation_date
endsnippet



snippet s3_delete_bucket
def delete_bucket(bucket_name):
	s3 = get_s3_resource()
	bucket = s3.Bucket(bucket_name)
	if bucket.creation_date:
		_ = [key.delete() for key in bucket.objects.all()]
		bucket.delete()
endsnippet


snippet s3_upload_resource
def upload_resource(path, bucket, object_name=None):
	if not os.path.exists(path):
		return None
	s3 = get_s3_resource()
	object_name = object_name or os.path.basename(path)
	s3.Bucket(bucket).upload_file(path, object_name)
endsnippet


snippet s3_upload
def upload(filepath, bucket, object_name=None):
	if not os.path.exists(path):
		return None
	object_name = object_name or os.path.basename(path)
	s3 = get_s3_client()
	s3.upload_file(filepath, bucket, object_name)
endsnippet


snippet s3_get_signed_url
def get_signed_url(bucket, object_name):
	s3 = get_s3_client()
	url = s3.generate_presigned_url(
		ClientMethod='get_object',
		Params={
			'Bucket': bucket,
			'Key': object_name,
		},
		ExpiresIn=None,
		HttpMethod=None,
	)
	return url
endsnippet


snippet s3_upload_from_buffer
def upload_from_buffer(client, bucket, path):
	buff = __import__('io').StringIO()
	buff.write('haha')
	resp = client.put_object(Body=buff.getvalue(), Bucket=bucket, Key=path)

endsnippet



#######################################################################
#                        Numpy - DATA SCIENCE                         #
#######################################################################

snippet random_matrix

endsnippet


#######################################################################
#                              UTILITIES                              #
#######################################################################
snippet url_change_port
import urllib
obj = urllib.parse.urlparse(url)
url = obj._replace(netloc=f'{obj.hostname}:${1:expected_port}').geturl()
endsnippet


snippet chunks
def chunks(l, size):
	for i in range(0, len(l), size):
		yield l[i:i+size]
endsnippet

snippet ichunks
def ichunks(generator, size):
	"""
	GOOD TO SPLIT A GENERATOR TO CHUNKS WITHOUT WALK THROUGH EVERY ITEM
	"""
	iterable_generator = iter(generator)
	while True:
		items = list(islice(iterable_generator, size))
		if not items:
		break
		yield items
endsnippet

snippet islice
def islice(maximum, chunk_size):
	for i in range(0, maximum - maximum % chunk_size + chunk_size, chunk_size):
		yield slice(i, min(i + chunk_size, maximum))
endsnippet

snippet meminfo
import psutil
def mem_info():
	used = format(psutil.virtual_memory().used / 1024 / 1024 / 1024, '.2f')
	total = format(psutil.virtual_memory().total / 1024 / 1024 / 1024, '.2f')
	percent = psutil.virtual_memory().percent
	logger.info('*'*15 + f'Current memory usage ({percent}%): [{used}GB / {total}GB]' + '*'*15)
	return percent
endsnippet

snippet oscheck
import psutil
def os_check(wait=2, timeout=60*60, mem_use=10, cpu_use=70, disk_free=5):
	"""
	:mem: Memory Usage (GB)
	:cpu: CPU Usage percentage
	:disk: Disk free size (GB)
	"""
	begin = time()
	mem, cpu, disk = get_os_usage()
	# mem_rank, cpu_rank = get_top_os_usage_processes()

	# while mem > mem_use or cpu > cpu_use or disk < disk_free:
	while mem > mem_use:
		if time() - begin > timeout:
			logger.critical('_'*15 + 'Memory usage is high for too long. Now quit waiting' + '_'*15)
			break  # Quit looping if it's waiting too long
		if mem > mem_use:
			gc.collect()
		if cpu > cpu_use:
			logger.info('#'*10 + f'CPU USAGE: {cpu}%.' + '#'*10)
		if disk < disk_free:
			logger.info('$'*10 + f'DISK FREE: {disk}GB' + '$'*10)
		sleep(wait)
		mem, cpu, disk = get_os_usage()  # Refresh current info
endsnippet

snippet osusage
import psutil
def get_os_usage():
	mem = psutil.virtual_memory().used / 1024 / 1024 / 1024  # GB
	cpu = mean(psutil.cpu_percent(interval=0.01, percpu=True))
	disk = psutil.disk_usage('/').free / 1024 / 1024 / 1024  # GB
	return mem, cpu, disk
endsnippet

snippet osusagetopprocesses
def get_top_os_usage_processes(top=1):
	top_mem_use, top_cpu_use = [], []
	# Iterate over all running process
	for proc in psutil.process_iter():
		try:
			pname = proc.name()
			mem_use = round(proc.memory_info().vms / 1024 / 1024, 2)  # MB
			# cpu_use = round(proc.cpu_percent() / psutil.cpu_count(), 2)
			cpu_use = max([proc.cpu_percent(interval=0.001) for i in range(1, 5)])
			top_mem_use.append(', '.join((pname, f'{mem_use} MB')))
			top_cpu_use.append(', '.join((pname, f'{cpu_use}%')))
		except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
			continue
	mem_rank = sorted(top_mem_use, reverse=True, key=lambda x: x[1])
	cpu_rank = sorted(top_cpu_use, reverse=True, key=lambda x: x[1])
	return mem_rank[:top], cpu_rank[:top]
endsnippet

snippet threadpool
from multiprocessing.pool import ThreadPool
pool = ThreadPool(concurrency)
responses = [pool.apply_async(${1:${VISUAL:func}}, args=(${2:${VISUAL:a}},)) for i in range(3)]
_ = [pool.close(), pool.join()]
results = [r.get() for r in responses]
endsnippet

snippet process-pool
responses = []
with ProcessPoolExecutor(max_workers=concurrency) as pool:
	responses = [pool.submit(${1:${VISUAL:func}}, *(arg1, arg2))]
results = [r.result() for r in responses]
endsnippet


snippet args-from-cli
import sys
import argparse
def get_sys_args():
	"""
	Refer: https://stackoverflow.com/a/15753721
		- CLI arguments: '--granularity daily weekly --dates 2019-01-01 2019-01-02_2019-01-31'
	"""
	parser = argparse.ArgumentParser()

	# Common args for all modes
	parser.add_argument('--choices', '-M', default='a', choices=['a', 'b', 'c'])
	parser.add_argument('--int', '-C', type=int, default=5)
	parser.add_argument('--bool', action='store_true', default=False)
	parser.add_argument('--str', type=str)
	parser.add_argument('--list-of-int', nargs='*', type=int, default=[])
	parser.add_argument('--list-of-str', nargs='*', type=int, default=[])
	parser.add_argument('--list-of-any', nargs='*', default=[])

	# Either from ENV or CLI args
	input_args = CI_PARAMS.get('CLI_ARGS', '').split() or sys.argv[1:]
	args = parser.parse_args(input_args)

	return args

sysargs = get_sys_args()
print(sysargs.choices)
endsnippet
