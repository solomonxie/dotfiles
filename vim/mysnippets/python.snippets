#######################################################################
#                               COMMON                                #
#######################################################################
snippet ifmain
def main():
	pass


if __name__ == '__main__':
	main()
endsnippet

snippet jsonload
with open('${1:${VISUAL:path}}', 'r') as f:
	${2:${VISUAL:data}} = json.loads(f.read())
endsnippet

snippet jsondump
with open('${1:${VISUAL:path}}', 'w') as f:
	f.write(json.dumps(${2:${VISUAL:data}}))
endsnippet

snippet parse-url
import urllib
urlobj = urllib.parse.urlparse(url)
endsnippet

snippet parse-url-params
import urllib
urlobj = urllib.parse.urlparse(url)
params = urllib.parse.parse_qs(urlobj.query)
endsnippet

snippet	shuffle_list
shuffled_list = sorted(${1:${VISUAL:target_list}}, key=lambda _: random.random())
endsnippet

snippet retry "from retry import retry"
@retry((KeyError, RuntimeError), tries=5, delay=2, jitter=5)
endsnippet

snippet deepcopy
from copy import deepcopy
endsnippet

snippet product
from itertools import product
endsnippet

snippet superi "execute super class init (py3)"
super().__init__()
endsnippet

#######################################################################
#                              DEBUGGING                              #
#######################################################################
snippet pdb
__import__('pdb').set_trace()
endsnippet

snippet ipdb
__import__('ipdb').set_trace()
endsnippet

snippet pp
__import__('pprint').pprint(${1})
endsnippet

snippet pprint
__import__('pprint').pprint(${1})
endsnippet

snippet sleep
__import__('time').sleep(${1})
endsnippet

snippet snoop
with __import__('pysnooper').snoop():
	${0:put_code_under_here_for_snooping}
endsnippet


snippet logger
from logging import getLogger
logger = getLogger(__name__)
endsnippet


#######################################################################
#                             DATE / TIME                             #
#######################################################################

snippet get_dates_from_range
from datetime import datetime, timedelta
def get_dates_from_range(granularity, start_date, end_date):
	"""
		print(get_dates_from_range('monthly', '2019-01-01', '2019-10-20'), '\n\n')
		print(get_dates_from_range('weekly', '2019-01-01', '2019-10-20'), '\n\n')
		print(get_dates_from_range('daily', '2019-01-01', '2019-01-20'), '\n\n')
		[('2018-12-30', 'weekly'), ('2019-01-06', 'weekly'), ('2019-01-01', 'daily')....]
	"""
	start, end = datetime.strptime(start_date, '%Y-%m-%d'), datetime.strptime(end_date, '%Y-%m-%d')
	daily_dates = [start + timedelta(days=i) for i in range((end - start).days + 1)]

	target_dates = []
	if granularity == 'daily':
		target_dates = daily_dates
	elif granularity == 'weekly':
		# Start from Monday: weeday_index = day.weekday()
		# Start from Sunday: weeday_index = (day.weekday() + 1) % 7
		target_dates = sorted(set([day - timedelta(days=(day.weekday() + 1) % 7) for day in daily_dates]))  # Sundays
	elif granularity == 'monthly':
		target_dates = sorted(set([day.replace(day=1) for day in daily_dates]))

	combos = [(day.strftime('%Y-%m-%d'), granularity) for day in target_dates]
	return combos
endsnippet

snippet get_start_end_date
from datetime import datetime

def get_start_end_date(target_date, granularity):
	target = datetime.strptime(target_date, '%Y-%m-%d') if isinstance(target_date, str) else target_date
	if granularity == 'daily':
		start = end = target
	elif granularity == 'weekly':
		start = target
		end = target + timedelta(days=6)
	elif granularity == 'monthly':
		start = target.replace(day=1)
		any_day_in_next_month = target.replace(day=28) + timedelta(days=4)
		end = any_day_in_next_month - timedelta(days=any_day_in_next_month.day)
	return start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d')
endsnippet

snippet now
from datetime import datetime, timedelta
_ = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
endsnippet

snippet from_datetime_str
from datetime import datetime
date_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
endsnippet

snippet to_datetime
from datetime import datetime
datetime_obj = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')
endsnippet


snippet epoch_to_datetime_str
from datetime import datetime
date_str = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
endsnippet


snippet datestr_to_epoch_rfc2822
from datetime import datetime

"""
RFC 2822 format: Tue, 26 Mar 2019 09:09:14 GMT
"""
date_time_obj = datetime.strptime(date_time_str, '%a, %d %b %Y %H:%M:%S %Z')
epoch_datetime = datetime(1970, 1, 1, 0, 0, 0)
epoch = int((date_time_obj - epoch).total_seconds())
endsnippet


snippet requests_get
import requests
from retry import retry

@retry((RequestException, ServerBusyError), tries=10, delay=30, jitter=5)
def requests_get(url, *args, **kwargs):
    try:
        resp = requests.get(url, *args, **kwargs)

    except Exception:
        msg = f'Error on requesting: {url}'
        raise RequestException(msg)

    if resp.status_code in [400, 403, 404, 428]:
        raise NoRetryError(f'BadRequest ({resp.status_code})')
    elif resp.status_code != 200:
        raise ServerBusyError(f'BadRequest ({resp.status_code})')

    # SubRequest Error
    result = resp.json() if resp.headers.get('Content-Type') == 'application/json' else {}
    if 'code' in result and result.get('code') in ['403', '428']:
        raise NoRetryError(f'Server SubRequest Issue ({result["code"]})')
    for error in result.get('errors') or []:
        if error.get('code') in ['SUB_REQUEST_TIMEOUT', '500', 'SUB_REQUEST_ERROR']:
            raise ServerBusyError(f'Server SubRequest Issue ({result["code"]})')

    return resp, result
endsnippet


snippet s3_operator
import os
import boto3

def get_s3_client(access_key, secret, endpoint=None, region=None):
    return boto3.client(
        's3',
        aws_access_key_id=access_key,
        aws_secret_access_key=secret,
        endpoint_url=endpoint,
        region_name=region,
        # config=boto3.session.Config(signature_version='s3v4'),
    )


def get_s3_resource(access_key, secret, endpoint=None, region=None):
    return boto3.resource(
        's3',
        aws_access_key_id=access_key,
        aws_secret_access_key=secret,
        endpoint_url=endpoint,
        region_name=region,
        # config=boto3.session.Config(signature_version='s3v4'),
    )


class S3Operator(object):
    def __init__(self, access_key=None, secret=None, endpoint=None, region=None):
        self.client = get_s3_client(
            access_key,
            secret,
            endpoint=endpoint,
            region=region
        )
        self.resource = get_s3_resource(
            access_key,
            secret,
            endpoint=endpoint,
            region=region
        )

    def upload_file(self, localpath, bucket_name, object_name=None):
        if not os.path.exists(localpath):
            return False
        object_name = object_name or os.path.basename(localpath)
        try:
            self.client.upload_file(localpath, bucket_name, object_name)
        except RuntimeError as ex:
            print(ex)
        return True

    def upload_file_blob(self, file_blob, bucket_name, object_name):
        try:
            self.client.put_object(Bucket=bucket_name, Key=object_name, Body=file_blob)
        except RuntimeError as ex:
            print(ex)
        return True

    def download_file_blob(self, bucket_name, object_name):
        obj = self.resource.Object(bucket_name, object_name)
        body = obj.get()
        blob = body['Body'].read()
        return blob

    def get_signed_url(self, bucket_name, object_name, expiredin=86400, httpmethod=None):
        url = self.client.generate_presigned_url(
            ClientMethod='get_object',
            Params={
                'Bucket': bucket_name,
                'Key': object_name,
            },
            ExpiresIn=expiredin,
            HttpMethod=httpmethod,
        )
        return url

    def create_bucket_if_not_exists(self, bucket_name):
        existing = [b['Name'] for b in self.client.list_buckets()['Buckets']]
        if bucket_name not in existing:
            self.resource.create_bucket(Bucket=bucket_name)
            print('Created bucket:', str(bucket_name))
        return self.resource.Bucket(bucket_name).creation_date

    def delete_bucket(self, bucket_name):
        """
        Only used in testing environment
        """
        existing = [b['Name'] for b in self.client.list_buckets()['Buckets']]
        if bucket_name in existing:
            bucket = self.resource.Bucket(bucket_name)
            _ = [key.delete() for key in bucket.objects.all()]
            bucket.delete()

    def list_objects_under_folder(self, bucket_name, folder_path=None):
        obj_keys = []
        is_truncated = False
        while is_truncated:
            objects = self.client.list_objects(Bucket=bucket_name, Prefix=folder_path, MaxKeys=1000)
            obj_keys += [obj['Key'] for obj in objects.get('Contents', [])]
            is_truncated = objects.get('IsTruncated', False)
        return obj_keys

    def list_objects(self, bucket_name, path_prefix=None):
        if path_prefix:
            keys = [obj.key for obj in self.resource.Bucket(bucket_name).objects.filter(Prefix=path_prefix)]
        else:
            keys = [obj.key for obj in self.resource.Bucket(bucket_name).objects.all()]
        return keys
endsnippet


snippet redis_conn
def get_redis_conn(db, redis_setting):
    xxxx
    return redis_conn


redis_conn = get_redis_conn('DB_NAME', redis_setting={'YOUR_REDIS_CONF': None})
redis_conn.hset('DB_NAME', 'YOUR_KEY', json.dumps({'YOUR_DATA': None}))
endsnippet


snippet requests_post
import requests
from retry import retry

@retry((RequestException, ServerBusyError), tries=10, delay=30, jitter=5)
def requests_post(url, data, *args, **kwargs):
    try:
        resp = requests.post(url, data=data, *args, **kwargs)

    except Exception:
        raise RequestException(f'Error on requesting: {url}.')

    if resp.status_code in [400, 403, 404, 428]:
        msg = f'BadRequest Request payload:\n{data}\nResponse content:{resp.content}\nResponse headers: {resp.headers}'
        raise NoRetryError(msg)
    elif resp.status_code != 200:
        raise ServerBusyError(f'Server Busy ({resp.status_code})')

    # SubRequest Error
    result = resp.json() if resp.headers.get('Content-Type') == 'application/json' else {}
    if 'code' in result and result.get('code') in ['403', '428']:
        raise NoRetryError(f'Server SubRequest Issue ({result["code"]})')
    for error in result.get('errors') or []:
        if error.get('code') in ['500', 'SUB_REQUEST_TIMEOUT', 'SUB_REQUEST_ERROR']:
            raise ServerBusyError(f'Server SubRequest Issue ({error["code"]})')

    return resp, result
endsnippet


snippet run_sql_to_pg_with_session
import sqlalchemy
from retry import retry
from contextlib import contextmanager
from sqlalchemy.pool import QueuePool
from sqlalchemy.orm import sessionmaker, scoped_session
from requests.structures import CaseInsensitiveDict

import settings

logger = getLogger(__name__)


SESSION_MAKER = {0: None}


def create_sqlalchemy_engine():
    conn_str = 'postgresql+psycopg2://{user}:{pwd}@{host}:{port}/{db}'.format(
        user=settings.DATABASES_RDS_USER,
        pwd=settings.DATABASES_RDS_PASSWORD,
        host=settings.DATABASES_RDS_HOST,
        port=settings.DATABASES_RDS_PORT,
        db=settings.DATABASES_RDS_NAME,
    )
    engine = sqlalchemy.create_engine(
        conn_str,
        # echo=settings.DATABASES_RDS_VERBOSE,
        echo=False,
        isolation_level='AUTOCOMMIT',  # IMPORTANT: R/O USER SHOULD HAVE THIS
        poolclass=QueuePool,
        pool_recycle=30,  # 1800sec on server | 30sec at local
        pool_size=5,
        connect_args={
            'application_name': 'bapi-service',
        },
    )
    return engine


def get_session_maker():
    """
    CONNECTION MANAGEMENT STRATEGY:
    - EACH THREAD HAS 1 ENGINE (CONNECTION POOL) TO KEEP THREAD SAFE
    - SESSION MAKER SHOULD BE AT TOP LEVEL IN THE APPLICATION
    """
    if SESSION_MAKER[0]:
        return SESSION_MAKER[0]

    logger.info('Creating new SQLALchemy Engine for RDS...')
    engine = create_sqlalchemy_engine()
    maker = sessionmaker(bind=engine, autocommit=True)
    SESSION_MAKER[0] = maker
    return maker


@contextmanager
def get_context_session():
    session = scoped_session(get_session_maker())
    try:
        yield session
    except psycopg2.OperationalError as ex:
        # Remove Global Session Maker if it's expired
        SESSION_MAKER[0] = None
        raise ex
    except Exception as ex:
        session.rollback()
        raise ex
    finally:
        session.close()


@retry((psycopg2.OperationalError, ), tries=5, delay=0.5, jitter=0.3, logger=logger)
def run_sql_to_pg_with_session(sql, params):
    """ SEPARATE ACTUAL IMPL FOR EASIER MOCK IN UT """
	""" THE ROW LIMIT SHOULD BE SPECIFIED IN SQL """
    start = time()
    with get_context_session() as session:
        logger.debug('RDS session created in {:.3f}sec'.format(time() - start))
        result = session.execute(sql, params).fetchall()
    rows = [CaseInsensitiveDict(x) for x in result]
    return rows
endsnippet


snippet run_sql_to_snowflake
import sqlalchemy
import sqlalchemy.orm
import snowflake.connector
from snowflake.connector import DictCursor
from requests.structures import CaseInsensitiveDict

import settings


def get_conn(schema):
    return snowflake.connector.connect(
        user=settings.SNOWFLAKE_USERNAME,
        password=settings.SNOWFLAKE_PASSWORD,
        account=settings.SNOWFLAKE_ACCOUNT,
        session_parameters={'QUERY_TAG': 'django-sf-poc'},
        warehouse=settings.SNOWFLAKE_WAREHOUSE,
        database=settings.SNOWFLAKE_DATABASE,
        schema=schema,
        # client_session_keep_alive=True,
    )


def run_sql(sql, params=None, schema=settings.SNOWFLAKE_SCHEMA_DIM, fetchone=False, use_dictcursor=True):
    conn = get_conn(schema=schema)
    cursor = conn.cursor(DictCursor) if use_dictcursor else conn.cursor()
    result = None
    try:
        cursor.execute(sql, params)
        result = cursor.fetchone() if fetchone else cursor.fetchall()
    finally:
        cursor.close()

    if use_dictcursor:
        if fetchone:
            result = CaseInsensitiveDict(result)
        else:
            result = [CaseInsensitiveDict(rec) for rec in result]

    return result


def get_snowflake_session(schema='ADL_MASTER', echo=False):
    """
    Ref: https://docs.snowflake.com/en/user-guide/sqlalchemy.html#additional-connection-parameters
    """
    conn_str = 'snowflake://{}:{}@{}/{}/{}?warehouse={}'.format(
        settings.SNOWFLAKE_USERNAME,
        settings.SNOWFLAKE_PASSWORD,
        settings.SNOWFLAKE_ACCOUNT,
        settings.SNOWFLAKE_DATABASE,
        schema,
        settings.SNOWFLAKE_WAREHOUSE,
    )
    engine = sqlalchemy.create_engine(conn_str, echo=echo)
    Session = sqlalchemy.orm.sessionmaker(bind=engine)
    session = Session()
    return session
endsnippet


#######################################################################
#                              FILE I/O                               #
#######################################################################
snippet csv_reader
with open('path', 'r') as f:
	reader = __import__('csv').DictWriter(f, fieldnames=cols)
	for row_dict in reader:
		print(row_dict)
endsnippet

snippet csv_writer
cols = ['col1', 'col2']
buff = __import__('io').StringIO()
writer = __import__('csv').DictWriter(buff, fieldnames=cols)
writer.writeheader()
writer.writerow({'name': ['psycho1', 'psycho2'], 'age': [10, 20]})
writer.writerows([{'name': 'jason', 'age': 18}, {'name': 'tom', 'age': 20}])
print(buff.getvalue())
endsnippet

snippet string_io
buff = __import__('io').StringIO()
buff.write('hello')
print(buffer.getvalue())
endsnippet

snippet buffer_io
buff = __import__('io').BytesIO()
buff.write(b'hello')
print(buffer.getvalue())
endsnippet

snippet file_io
endsnippet

snippet gzip_compress
content = __import__('gzip').compress(b'${1:${VISUAL:hello}}', compresslevel=9)
endsnippet


#######################################################################
#                           TEST FRAMEWORK                            #
#######################################################################
snippet mock_constant
patch('${1:module.module.constant_name}', '${2:mocked_value}').start()
endsnippet

snippet mock_method
patch('${1:module.module.method}', MagicMock(return_value=${2:mocked_value})).start()
endsnippet

snippet mock_property
patch(${1:module.module.class_name}, '${2:property_name}', ${3:expected_value}).start()
endsnippet

snippet mock_exception
patch('${1:module.module.method_name}', MagicMock(side_effect=Exception('${2:err_msg}'))).start()
endsnippet


snippet testcase
from unittest import TestCase


class Test${1:${VISUAL:Name}}(TestCase):
	def setUp(self):
		pass

	def tearDown(self):
		pass

	def test_${2:${VISUAL:case_name}}(self):
		self.assertEqual(1, 1)
endsnippet


snippet patch
from unittest.mock import patch, MagicMock
@patch('${1:${VISUAL:module_path}}',
		MagicMock(return_value=${2:${VISUAL:mock_data}}))
endsnippet

#######################################################################
#                     SQLAlchemy - DB OPERATIONS                      #
#######################################################################



#######################################################################
#                         Pandas - DATAFRAME                          #
#######################################################################



#######################################################################
#                              AWS - S3                               #
#######################################################################

snippet s3_client
import boto3
def get_s3_client():
	return boto3.client(
		's3',
		aws_access_key_id='${1:{VISUAL:key_id}}',
		aws_secret_access_key='${2:{VISUAL:secret}}',
		# config=boto3.session.Config(signature_version='s3v4'),
		# endpoint_url='http://your-own-endpoint',
		# region_name='your-region',
	)
endsnippet

snippet s3_resource
import boto3
def get_s3_resource():
	return boto3.resource(
		's3',
		aws_access_key_id='${1:{VISUAL:key_id}}',
		aws_secret_access_key='${2:{VISUAL:secret}}',
		# config=boto3.session.Config(signature_version='s3v4'),
		# endpoint_url='http://your-own-endpoint',
		# region_name='your-region',
	)
endsnippet


snippet s3_create_bucket_if_not_exist
def create_bucket_if_not_exists(bucket):
	s3 = get_s3_resource()
	if not s3.Bucket(bucket).creation_date:
		info = s3.create_bucket(Bucket=bucket)
		print('Created bucket:', info)
	return s3.Bucket(bucket).creation_date
endsnippet



snippet s3_delete_bucket
def delete_bucket(bucket_name):
	s3 = get_s3_resource()
	bucket = s3.Bucket(bucket_name)
	if bucket.creation_date:
		_ = [key.delete() for key in bucket.objects.all()]
		bucket.delete()
endsnippet


snippet s3_upload_resource
def upload_resource(path, bucket, object_name=None):
	if not os.path.exists(path):
		return None
	s3 = get_s3_resource()
	object_name = object_name or os.path.basename(path)
	s3.Bucket(bucket).upload_file(path, object_name)
endsnippet


snippet s3_upload
def upload(filepath, bucket, object_name=None):
	if not os.path.exists(path):
		return None
	object_name = object_name or os.path.basename(path)
	s3 = get_s3_client()
	s3.upload_file(filepath, bucket, object_name)
endsnippet


snippet s3_get_signed_url
def get_signed_url(bucket, object_name):
	s3 = get_s3_client()
	url = s3.generate_presigned_url(
		ClientMethod='get_object',
		Params={
			'Bucket': bucket,
			'Key': object_name,
		},
		ExpiresIn=None,
		HttpMethod=None,
	)
	return url
endsnippet


snippet s3_upload_from_buffer
def upload_from_buffer(client, bucket, path):
	buff = __import__('io').StringIO()
	buff.write('haha')
	resp = client.put_object(Body=buff.getvalue(), Bucket=bucket, Key=path)

endsnippet



#######################################################################
#                        Numpy - DATA SCIENCE                         #
#######################################################################

snippet random_matrix

endsnippet


#######################################################################
#                              UTILITIES                              #
#######################################################################
snippet url_change_port
import urllib
obj = urllib.parse.urlparse(url)
url = obj._replace(netloc=f'{obj.hostname}:${1:expected_port}').geturl()
endsnippet


snippet chunks
def chunks(items, size):
	for i in range(0, len(items), size):
		yield items[i: i+size]
endsnippet

snippet ichunks
def ichunks(generator, size):
	"""
	GOOD TO SPLIT A GENERATOR TO CHUNKS WITHOUT WALK THROUGH EVERY ITEM
	"""
	iterable_generator = iter(generator)
	while True:
		items = list(islice(iterable_generator, size))
		if not items:
		break
		yield items
endsnippet


snippet mkdir
import os

def mkdir(path):
	dirname = os.path.dirname(path)
	if not os.path.exists(dirname):
		try:
			os.makedirs(dirname)
		except Exception:
			pass
endsnippet

snippet islice
def islice(maximum, chunk_size):
	for i in range(0, maximum - maximum % chunk_size + chunk_size, chunk_size):
		yield slice(i, min(i + chunk_size, maximum))
endsnippet

snippet meminfo
import psutil
def mem_info():
	used = format(psutil.virtual_memory().used / 1024 / 1024 / 1024, '.2f')
	total = format(psutil.virtual_memory().total / 1024 / 1024 / 1024, '.2f')
	percent = psutil.virtual_memory().percent
	print('*'*15 + f'Current memory usage ({percent}%): [{used}GB / {total}GB]' + '*'*15)
	return percent
endsnippet

snippet oscheck
import psutil
def os_check(wait=2, timeout=60*60, mem_use=10, cpu_use=70, disk_free=5):
	"""
	:mem: Memory Usage (GB)
	:cpu: CPU Usage percentage
	:disk: Disk free size (GB)
	"""
	begin = time()
	mem, cpu, disk = get_os_usage()
	# mem_rank, cpu_rank = get_top_os_usage_processes()

	# while mem > mem_use or cpu > cpu_use or disk < disk_free:
	while mem > mem_use:
		if time() - begin > timeout:
			print('_'*15 + 'Memory usage is high for too long. Now quit waiting' + '_'*15)
			break  # Quit looping if it's waiting too long
		if mem > mem_use:
			gc.collect()
		if cpu > cpu_use:
			print('#'*10 + f'CPU USAGE: {cpu}%.' + '#'*10)
		if disk < disk_free:
			print('$'*10 + f'DISK FREE: {disk}GB' + '$'*10)
		sleep(wait)
		mem, cpu, disk = get_os_usage()  # Refresh current info
endsnippet

snippet osusage
import psutil
def get_os_usage():
	mem = psutil.virtual_memory().used / 1024 / 1024 / 1024  # GB
	cpu = mean(psutil.cpu_percent(interval=0.01, percpu=True))
	disk = psutil.disk_usage('/').free / 1024 / 1024 / 1024  # GB
	return mem, cpu, disk
endsnippet

snippet osusagetopprocesses
def get_top_os_usage_processes(top=1):
	top_mem_use, top_cpu_use = [], []
	# Iterate over all running process
	for proc in psutil.process_iter():
		try:
			pname = proc.name()
			mem_use = round(proc.memory_info().vms / 1024 / 1024, 2)  # MB
			# cpu_use = round(proc.cpu_percent() / psutil.cpu_count(), 2)
			cpu_use = max([proc.cpu_percent(interval=0.001) for i in range(1, 5)])
			top_mem_use.append(', '.join((pname, f'{mem_use} MB')))
			top_cpu_use.append(', '.join((pname, f'{cpu_use}%')))
		except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
			continue
	mem_rank = sorted(top_mem_use, reverse=True, key=lambda x: x[1])
	cpu_rank = sorted(top_cpu_use, reverse=True, key=lambda x: x[1])
	return mem_rank[:top], cpu_rank[:top]
endsnippet

snippet multiprocess
from multiprocessing import Process
params_list = []
tasks = [Process(target=func, args=(params,)) for params in params_list]
_ = [t.start() for t in tasks]
_ = [t.join() for t in tasks]
_ = [t.close() for t in tasks]
endsnippet

snippet threadpool
from multiprocessing.pool import ThreadPool
pool = ThreadPool(concurrency)
responses = [pool.apply_async(${1:${VISUAL:func}}, args=(${2:${VISUAL:a}},)) for i in range(3)]
_ = [pool.close(), pool.join()]
results = [r.get() for r in responses]
endsnippet

snippet process-pool
responses = []
with ProcessPoolExecutor(max_workers=concurrency) as pool:
	responses = [pool.submit(${1:${VISUAL:func}}, *(arg1, arg2))]
results = [r.result() for r in responses]
endsnippet

snippet thread_task_pool
from multiprocessing.pool import ThreadPool
MAX_THREAD_COUNT = 100
task_pool = ThreadPool(MAX_THREAD_COUNT)

def run_in_threads(task_list):
    results = {}
    for i, (func, params) in enumerate(task_list):
        results[i] = task_pool.apply_async(func, args=params)

    return [results[i] for i in sorted(results.keys())]

def test_run_threads():
    task_list = [
        (sorted, ([3, 2, 1], )),
        (max, ([1, 2, 3], ))
    ]
    results = run_in_threads(task_list)
    results = [r.get() for r in results]
    order, big = results
    print(order)
    print(big)
endsnippet

snippet cross_process_share
from multiprocessing import Manager

m = Manager()
IN_MEM_INFO = m.dict()


def update_in_mem_info(key: str, new_info: dict):
	# SHARED INFO ACCROSS PROCESSES
	""" In-mem sharing dict can't be updated in sub-process but directly assign the value """
	into = IN_MEM_INFO.get(key) or {}
	into.update(new_info)
	IN_MEM_INFO[task.task_id] = info
endsnippet

snippet timeout
import signal

def timeout(func):
    """ DOESN'T WORK IN THREADS """
    TTL = 10

    def handler(_, frame):
        raise RuntimeError('[ TIMEOUT ] FORCE QUIT: {}'.format(str(frame)))

    def wrapper(*args, **kwargs):
        signal.signal(signal.SIGALRM, handler)
        signal.alarm(TTL)
        try:
            result = func(*args, **kwargs)
        except RuntimeError:
            result = None
            print('[ TIMEOUT ] Force shutdown function: {}'.format(str(func)))
        finally:
            signal.alarm(0)
        return result

    return wrapper
endsnippet


snippet args
import sys
import argparse
def get_sys_args():
	"""
	Refer: https://stackoverflow.com/a/15753721
		- CLI arguments: '--granularity daily weekly --dates 2019-01-01 2019-01-02_2019-01-31'
	"""
	parser = argparse.ArgumentParser()

	# Common args for all modes
	parser.add_argument('--choices', '-M', default='a', choices=['a', 'b', 'c'])
	parser.add_argument('--int', '-C', type=int, default=5)
	parser.add_argument('--bool', action='store_true', default=False)
	parser.add_argument('--str', type=str)
	parser.add_argument('--list-of-int', nargs='*', type=int, default=[])
	parser.add_argument('--list-of-str', nargs='*', type=int, default=[])
	parser.add_argument('--list-of-any', nargs='*', default=[])

	# Either from ENV or CLI args
	input_args = CI_PARAMS.get('CLI_ARGS', '').split() or sys.argv[1:]
	args = parser.parse_args(input_args)

	return args

sysargs = get_sys_args()
print(sysargs.choices)
endsnippet


snippet defaultdict_lambda
from collections import defaultdict
_ = defaultdict(lambda: defaultdict(set))
endsnippet

snippet sorted_key
_ = sorted(items, key=lambda x: x['name'], reverse=True)
endsnippet


snippet cacheout_memoize
# pip install cacheout==0.11.2
from cacheout import Cache

cache_for_func = Cache(5120, ttl=settings.BREADCRUMB_TTL)

@cache_for_func.memoize()
def my_func(myid, myname=None):
	pass
endsnippet

snippet filelock
# pip install fielock
from filelock import Filelock

lock = FileLock('/tmp/some.lock')

for n in range(5):
    with lock.acquire(timeout=10, poll_intervall=0.01):
        open('/tmp/haha', 'a').write(str(n)*5 + '\n')
endsnippet



snippet testcase_ft_flask
import json
import pook
from unittest import TestCase
from unittest.mock import patch

from application.app import app
from tests.interface.test_utils import mock_run_sql, mock_engine
from tests import sqls


class TestBreadcrumb(TestCase):
    def setUp(self):
        self.client = app.app.app.test_client()
        pook.on()
        self.patches = [
            patch('db_utils.run_sql', side_effect=mock_run_sql),
        ]
        for p in self.patches:
            p.start()

        # Prepare mock data
        engine = mock_engine()
        print('\n[ ... ] INSERT SAMPLE DATA FOR UT')
        with engine.connect() as conn:
            conn.execute(sqls.create_tables.sql)
            conn.execute(sqls.insert_records.sql)
            conn.execute('select * from mytable limit 1')
        print('[ OK ] INSERT SAMPLE DATA DONE.')

		# Setup Context
		self.context = flask.Flask(__name__).test_request_context()
        self.context.push()
        setattr(flask.request, 'request_id', 'fake_req_id')
        seq_id_gen = MagicMock()
        seq_id_gen.sequence_id = 'fake_seq_id'
        setattr(flask.request, 'sequence_id_generator', seq_id_gen)

    def tearDown(self):
        engine = mock_engine()
        with engine.connect() as conn:
            conn.execute(sqls.clean_tables.sql)

        pook.off()
        pook.reset()
        self.context.pop()
        for p in self.patches:
            p.stop()

    def test_endpoint(self):
		# Mock API calls inside of the impl
        resp_d = {'data': [{'token': 'fake_token'}]}
        pook.get(url='http://localhost/get-tokens?user_id=1', reply=200, response_json=resp_d)

        url = '/api/myendpoint/?myid=12345'
        resp = self.client.get(url)
        self.assertEqual(200, resp.status_code)
        result = json.loads(resp.data)
        self.assertIsNotNone(result)
endsnippet


snippet pook_get
import pook
import json
pook.on()
(pook.get('http://localhost/myendpoint?q=1')
	 .reply(200)
	 .headers({'Content-Type': 'application/json'})
	 .json(json.dumps({'data': [1, 2, 3]}))
)
pook.off()
pook.reset()
endsnippet

snippet pook_post
import pook
import json
(pook.post('http://localhost/myendpoint?q=1')
	 .type('json')  # Request body type
	 .headers({'Content-Type': 'application/json'})  # Request header
	 .reply(200)
	 .headers({'Content-Type': 'application/json'})  # Response header
	 .json(json.dumps({'data': [1, 2, 3]}))
)
endsnippet


snippet flatten_deep_dict
def flatten_deep_dict(data, path='root', sort=True, sort_keys=['id', 'type', 'genre_id', 'modifier_id']):
    new = {}
    if not isinstance(data, (dict, list)):
        return data
    keys = list(data.keys()) if isinstance(data, dict) else list(range(len(data)))
    for k in keys:
        item = data[k]
        sub_path = path + '.' + str(k)
        if isinstance(item, (dict, list)):
            # Dict Path
            for sk in sort_keys:
                if isinstance(item, dict) and sk in item:
                    sub_path = path + '.' + str(sk) + '-' + str(item[sk])
                    break
            # Sort List
            if sort and isinstance(item, list) and len(item) and not isinstance(item[0], (dict, list)):
                item = sorted(item)
            sub = flatten_deep_dict(item, sub_path)
        else:
            sub = {sub_path: item}
        new.update(sub)
    return new
endsnippet


snippet counter
from collections import Counter
anything = [1, 2, 3, 3]  # tuple, list, dict, string... anything
result_dict = Counter(anything)
print(result_dict)
# {'target1': num_of_appearances, 'target2': num_of_appearances}
endsnippet


snippet dtree
class Dtree(dict):
    """ ALLOWS TREE LIKE DEEP DICT TO REPLACE DEFAULTDICT FOR EASIER CACHE """
    def __missing__(self, key):
        value = self[key] = type(self)()
        return value
endsnippet

snippet decorator
from functools import wraps, partial
import logging


def logged(func=None, *d_args, level=logging.DEBUG, name=None, message=None, **d_kwargs):
    if func is None:
		# 1. With dec-args, func == None 2. Without dec-args, func != None
		# Here it's executed while "decorating", not while func "running"
        return partial(logged, *d_args, level=level, name=name, message=message, **d_kwargs)

    logname = name if name else func.__module__
    log = logging.getLogger(logname)
    logmsg = message if message else func.__name__

    @wraps(func)
    def wrapper(*args, **kwargs):
        __import__('pudb').set_trace()
        log.log(level, logmsg)
        return func(*args, **kwargs)

    return wrapper


@logged  # func == add
def add(x, y):
    __import__('pudb').set_trace()
    return x + y


@logged(level=logging.CRITICAL, name='example')  # func == None
def spam():
    __import__('pudb').set_trace()
    print('Spam!')
endsnippet
